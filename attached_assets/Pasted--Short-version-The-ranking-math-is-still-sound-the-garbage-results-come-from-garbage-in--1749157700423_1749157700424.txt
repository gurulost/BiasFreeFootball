### Short version

The **ranking math is still sound**; the garbage results come from **garbage in, plus one accidental code tweak**:

1. **API data contamination** – the new `apinext` host stopped honoring `division=fbs`, so FCS (and even D-II) rows flooded in.
2. **Two FBS teams silently dropped** – a filtering hiccup trimmed the roster to 132 schools.
3. **A helper function re-normalised the PageRank matrix** -– that cut every rating roughly in half.

None of those change the *theory* of the algorithm, but they wreck its inputs (and, in the third case, its numerical scale).

---

### How it went off-track

| Stage             | What used to happen                         | What changed                                                                                                 |
| ----------------- | ------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Fetch teams**   | `teams?year=2024&division=fbs` → 134 rows   | Same call on **apinext** now returns 250+ rows; filter wasn’t re-checked.                                    |
| **Fetch games**   | Same FBS filter, so only FBS vs FBS         | Filter ignored, so every CFBD row for 2024 came down.                                                        |
| **Graph build**   | 134 nodes, \~850 edges                      | > 200 nodes, \~1 400 edges, two FBS schools missed ⇒ 132 nodes left.                                         |
| **PageRank step** | Supply weighted adjacency raw → nx.pagerank | A refactor added a pre-normalise `A = A / A.sum(axis=1)`; nx.pagerank does that again → ratings scaled down. |

Because those changes happened quietly (no tests on team count, rating scale, or FCS bleed-through), the site *looked* like it was working until someone noticed Miami-OH and WKU sitting above Alabama.

---

### So is it “algo” or “data”?

*Mostly data.* The extra normalise line is a small implementation slip, not a design change. The rest is pure input pollution caused by a vendor-side API change that our ingest layer didn’t guard against.

---

### One-line test that would have caught it

```python
assert len(fbs_teams_2024) == 134, "Team count off → check API filter"
```

Add that (plus a top-rating floor check) and CI will scream the next time CFBD changes behaviour.

---

### Bottom line for the team

* Re-enable a **hard whitelist** of FBS team IDs (built from `teams?year=…`).
* Strip the accidental pre-normalisation.
* Re-run: top rating pops back to \~0.014, power leagues dominate top 15, BYU in top 25, “quality wins” column shows real opponents.

Once those two fixes are in, the algorithm’s original logic produces the same healthy output you saw earlier.
