Below is a hardening checklist you can hand straight to the dev team.  If every item is in place, data-quality issues will surface immediately and *never* propagate into the ranking math.

---

### 1 — Lock down the raw feed

* **Pin an API version**
  Always call the CollegeFootballData `vX.Y` endpoint you’ve tested.  If the vendor revs the schema, you get a 4xx and CI fails instead of silent mis-parsing.

* **Record fetch hash**
  After every pull, write `sha256(raw_json)` to `data/raw/_checksums.txt`.  Re-fetch the same URL in a dry-run step and assert the hash hasn’t changed.  If it does, the vendor retro-edited history—fail the build so we notice.

---

### 2 — Schema and type validation

Run every game row through `pydantic` or `marshmallow`:

```python
class Game(BaseModel):
    season: int
    week: int
    home_team: constr(strip_whitespace=True)
    away_team: constr(strip_whitespace=True)
    home_points: conint(ge=0)
    away_points: conint(ge=0)
    neutralSite: bool
    season_type: Literal["regular", "postseason"]
    # …etc.
```

Any missing or wrong-typed field raises immediately.

---

### 3 — Canonical map round-trip

*After* alias resolution, confirm that re-looking-up the canonical name lands on itself:

```python
assert canonicalize(canonical_name)['name'] == canonical_name
```

Guarantees you never introduce a new alias by accident.

---

### 4 — Completeness & duplication audits

* **Game-count assertion** (already added).
* **Duplicate detection**
  Build a `game_id = (season, week, home_team, away_team)` set; assert uniqueness. Duplicates creep in during bowl reschedules.

---

### 5 — Outlier guardrails

Compute quick stats *before* the heavy graph build:

```python
margins = [abs(g.home_points - g.away_points) for g in games]
assert np.percentile(margins, 99) < 70, "suspicious 100-point blowout?"
```

Similarly sanity-check:

* points > 150
* tie games in post-2005 seasons (overtime era)
  Anything impossible prints and aborts.

---

### 6 — Golden-file tests

Store a tiny frozen dataset (`tests/golden_2019_week01.json`) whose correct
conference strengths and top-10 rankings are in a fixture.
A unit test re-runs the full pipeline on that file and
asserts the rating vector matches to 1e-9.
If a future code change or library upgrade shifts math, CI turns red.

---

### 7 — Runtime anomaly monitor

After every weekly run:

* **Edge-weight histogram** – log mean, σ, max.  Sudden spikes mean a margin cap or multiplier went rogue.
* **Rating z-scores** – flag any team that moves > 4σ week-to-week; print their edge breakdown.

Push these diagnostics to the Action log; they cost pennies but let you spot weirdness before a user does.

---

### 8 — Redundant storage

Write cleaned season CSVs to an S3 bucket (or Git LFS) with content hashes in the filename.
If the vendor API is down on Sunday, the pipeline falls back to the cached file—rankings still publish.

---

### 9 — CI policy

* All validation, alias, and completeness tests run in pull-request CI.
* Main branch is **protected**: no merge if *any* data-quality test fails.
* Scheduled Sunday Action begins with `git pull --rebase origin main` so emergency fixes merged after Friday are included automatically.

---

### 10 — Human lint pass once per season

At preseason and after bowls, generate:

```
reports/season_YYYY_team_summary.csv
```

with per-team totals (games, wins, losses, avg margin).  A 15-line
diff view in GitHub will show if any team has 13 games when it should have 12.

---

Put these ten layers in place and your ranking math will only ever see
verified, canonical, non-duplicated, schema-correct game rows.  From that point, any odd ranking is diagnostic of the model, not the data.
